{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "\n",
    "### Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From OpenAI GitHub Repo [here](https://github.com/openai/gym/blob/4c460ba6c8959dd8e0a03b13a1ca817da6d4074f/gym/envs/toy_text/discrete.py#L16)  \n",
    "Discrete Environment Variables:\n",
    "```python\n",
    "- nS: number of states\n",
    "- nA: number of actions\n",
    "- P: transitions (*)\n",
    "- isd: initial state distribution (**)\n",
    "(*) dictionary dict of dicts of lists, where P[s][a] == [(probability, nextstate, reward, done), ...]\n",
    "(**) list or array of length nS\n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(env):\n",
    "    \"\"\"\n",
    "    env: OpenAI Gym Environment\n",
    "    \"\"\"\n",
    "    values = np.zeros(env.nS)\n",
    "    policy = np.zeros((env.nS, env.nA))\n",
    "    return values, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, values, discount, theta):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS): # For every state\n",
    "            value = values[s]\n",
    "            new_value = 0\n",
    "            for a in range(env.nA): # For every action\n",
    "                for transition, nextstate, reward, done in env.P[s][a]: # For every next state when action a is taken\n",
    "                    new_value += transition * (reward + discount * values[nextstate]) # Bellman optimality equation\n",
    "            delta = max(delta, np.abs(value-new_value))\n",
    "            values[s] = new_value\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, values, policy, discount):\n",
    "    policy_stable = True\n",
    "    for s in range(env.nS):\n",
    "        old_action = np.argmax(policy[s])\n",
    "        action_values = []\n",
    "        for a in range(env.nA):\n",
    "            action_value = 0\n",
    "            for transition, nextstate, reward, done in env.P[s][a]:\n",
    "                action_value += transition * (reward + discount * values[nextstate]) # Bellman optimality\n",
    "            action_values.append(action_value)\n",
    "        new_action = np.argmax(action_values) # Since we are dealing with policy take max action instead of summing them\n",
    "        new_probs = np.zeros(env.nA)\n",
    "        new_probs[new_action] += 1.0\n",
    "        policy[s] = new_probs\n",
    "        if old_action != new_action:\n",
    "            policy_stable = False\n",
    "    return policy_stable, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, discount=0.9, theta=0.001):\n",
    "    policy_stable = False\n",
    "    values, policy = init(env)\n",
    "    while not policy_stable:\n",
    "        values = policy_evaluation(env, values, discount, theta)\n",
    "        policy_stable, policy = policy_improvement(env, values, policy, discount)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = FrozenLakeEnv()\n",
    "policy_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
