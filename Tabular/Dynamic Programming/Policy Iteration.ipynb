{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "\n",
    "## Background\n",
    "Policy iteration is the process of iterating over all possible states and trying to improve the policy at the end of each pass through the states. For this we need to first evaluate (predict) the current level of policies and then update the policy if it is not good enough. This approach assumes that we know everything about the environment (not very realistic) and discrete (tabular) states.\n",
    "\n",
    "### Policy Evaluation\n",
    "For each state, we can use the Bellman Equation for iterative update of state values.  \n",
    "\n",
    "<center>\n",
    "$v_{k+1}(s) = \\sum\\limits_{a}\\pi (a \\mid s)\\sum\\limits_{s'}p(s',r\\mid s,a)[r+\\gamma v_k(s')]$\n",
    "</center>  \n",
    "\n",
    "This replaces the old values of $s$ with the new expected returns for all possible actions and successor sates of $s$.  We can repeat this until the change in values is smaller than a constant, i.e:\n",
    "<center>\n",
    "$\\Delta = \\max(\\Delta, \\mid v_{k+1}(s)-v_{k}(s)\\mid)$  \n",
    "$\\Delta \\stackrel{?}{<} \\Theta$\n",
    "</center>\n",
    "\n",
    "### Policy Update\n",
    "Now that we have the new state values, we can update the policy accordingly. We will use a very similar approach with one difference. Since policy is the probability of picking an action given a state, we want to update this probability according to the new values. Therefore, we will pick the maximum action that makes the value from the current state maximum. We can also do this iteretivaly as well.  \n",
    "\n",
    "<center>\n",
    "$\\pi(s) = \\underset{a}{\\operatorname{argmax}}\\sum\\limits_{s'}p(s',r\\mid s,a)[r+\\gamma v_k(s')]$\n",
    "</center>\n",
    "\n",
    "### Combined\n",
    "Rest is easy. Repeat Policy Evaluation + Policy Update until the policy won't change. It can be proven that the resulting policy and state values are actually $v^*$ and $\\pi^*$.  \n",
    "\n",
    "<center>\n",
    "    $\\pi_0\\stackrel{E}{\\rightarrow}v_1\\stackrel{I}{\\rightarrow}\\pi_1\\stackrel{E}{\\rightarrow}...\\stackrel{I}{\\rightarrow}\\pi^*\\stackrel{E}{\\rightarrow}v^*$\n",
    "</center>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From OpenAI GitHub Repo [here](https://github.com/openai/gym/blob/4c460ba6c8959dd8e0a03b13a1ca817da6d4074f/gym/envs/toy_text/discrete.py#L16)  \n",
    "Discrete Environment Variables:\n",
    "```python\n",
    "- nS: number of states\n",
    "- nA: number of actions\n",
    "- P: transitions (*)\n",
    "- isd: initial state distribution (**)\n",
    "(*) dictionary dict of dicts of lists, where P[s][a] == [(probability, nextstate, reward, done), ...]\n",
    "(**) list or array of length nS\n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(env):\n",
    "    \"\"\"\n",
    "    env: OpenAI Gym Environment\n",
    "    \"\"\"\n",
    "    values = np.zeros(env.nS)\n",
    "    policy = np.zeros((env.nS, env.nA))\n",
    "    return values, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, values, policy, discount, theta):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS): # For every state\n",
    "            value = values[s]\n",
    "            new_value = 0\n",
    "            for a in range(env.nA): # For every action\n",
    "                for transition, nextstate, reward, done in env.P[s][a]: # For every next state when action a is taken\n",
    "                    transition *= policy[s][a] # Don't forget we need p(s',r|s,pi(s))\n",
    "                    new_value += transition * (reward + discount * values[nextstate]) # Bellman optimality equation\n",
    "            delta = max(delta, np.abs(value-new_value))\n",
    "            values[s] = new_value\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, values, policy, discount):\n",
    "    policy_stable = True\n",
    "    for s in range(env.nS):\n",
    "        old_action = np.argmax(policy[s])\n",
    "        action_values = []\n",
    "        for a in range(env.nA):\n",
    "            action_value = 0\n",
    "            for transition, nextstate, reward, done in env.P[s][a]:\n",
    "                action_value += transition * (reward + discount * values[nextstate]) # Bellman optimality\n",
    "            action_values.append(action_value)\n",
    "        new_action = np.argmax(action_values) # Since we are dealing with policy take max action instead of summing them\n",
    "        new_probs = np.zeros(env.nA)\n",
    "        new_probs[new_action] += 1.0\n",
    "        policy[s] = new_probs\n",
    "        if old_action != new_action:\n",
    "            policy_stable = False\n",
    "    return policy_stable, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, discount=0.9, theta=0.0001):\n",
    "    policy_stable = False\n",
    "    values, policy = init(env)\n",
    "    while not policy_stable:\n",
    "        values = policy_evaluation(env, values, policy, discount, theta)\n",
    "        policy_stable, policy = policy_improvement(env, values, policy, discount)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = FrozenLakeEnv()\n",
    "policy = policy_iteration(env)\n",
    "done = False\n",
    "state = env.reset()\n",
    "env.render()\n",
    "rewards = []\n",
    "while not done:\n",
    "    state, reward, done, _ = env.step(np.argmax(policy[state]))\n",
    "    rewards.append(reward)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10a0ca0b8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a0ca080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards, label=\"Reward\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
